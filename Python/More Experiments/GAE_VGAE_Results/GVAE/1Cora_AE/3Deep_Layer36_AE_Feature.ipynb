{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Import defined methods\n",
    "import sys\n",
    "sys.path.append(r'/home/xinxingwu/Done_Results/GAE_VGAE_Results/GVAE_Pubmed_36')\n",
    "\n",
    "from linear_gae.evaluation import get_roc_score\n",
    "from linear_gae.input_data import load_data, load_label\n",
    "from linear_gae.model import *\n",
    "from linear_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from linear_gae.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(p_data,p_path):\n",
    "    dataframe = pd.DataFrame(p_data)\n",
    "    dataframe.to_csv(p_path, mode='a',header=False,index=False,sep=',')\n",
    "    del dataframe\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "output_path=\"/home/xinxingwu/Done_Results/GAE_VGAE_Results/GVAE_Pubmed_36/1Cora_AE/log/\"\n",
    "dataset='cora'\n",
    "task='link_prediction'\n",
    "model_name='deep36_gcn_vae'\n",
    "dropout=0.\n",
    "epochs=200\n",
    "features_used=True\n",
    "learning_rate=0.01\n",
    "nb_run=10 # Number of model run + test\n",
    "prop_val=5 # Proportion of edges in validation set (for Link Prediction task)\n",
    "prop_test=10 # Proportion of edges in test set (for Link Prediction task)\n",
    "validation=True # Whether to report validation results at each epoch (for Link Prediction task)\n",
    "verbose=True # Whether to print comments details\n",
    "kcore=False # Whether to run k-core decomposition and use the framework. False = model will be trained on the entire graph\n",
    "task='link_prediction'\n",
    "\n",
    "p_model_name=model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "mean_time = []\n",
    "if verbose:\n",
    "    print(\"Loading data...\")\n",
    "adj_init, features_init = load_data(dataset)\n",
    "\n",
    "# Lists to collect average results\n",
    "if task == 'link_prediction':\n",
    "    mean_roc = []\n",
    "    mean_ap = []\n",
    "\n",
    "mean_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 1.72544 time= 1.98080\n",
      "val_roc= 0.65593 val_ap= 0.70349\n",
      "Epoch: 0002 train_loss= 1.66777 time= 0.61391\n",
      "val_roc= 0.66419 val_ap= 0.71359\n",
      "Epoch: 0003 train_loss= 1.71794 time= 0.83127\n",
      "val_roc= 0.67851 val_ap= 0.72016\n",
      "Epoch: 0004 train_loss= 1.70761 time= 1.65271\n",
      "val_roc= 0.67554 val_ap= 0.70212\n",
      "Epoch: 0005 train_loss= 1.73308 time= 0.82219\n",
      "val_roc= 0.67408 val_ap= 0.69849\n",
      "Epoch: 0006 train_loss= 1.75088 time= 0.81434\n",
      "val_roc= 0.67404 val_ap= 0.71201\n",
      "Epoch: 0007 train_loss= 1.71176 time= 0.95697\n",
      "val_roc= 0.67852 val_ap= 0.72105\n",
      "Epoch: 0008 train_loss= 1.72213 time= 1.72106\n",
      "val_roc= 0.67877 val_ap= 0.72511\n",
      "Epoch: 0009 train_loss= 1.70323 time= 1.32244\n",
      "val_roc= 0.67901 val_ap= 0.72601\n",
      "Epoch: 0010 train_loss= 1.71402 time= 0.69609\n",
      "val_roc= 0.67916 val_ap= 0.72635\n",
      "Epoch: 0011 train_loss= 1.67607 time= 2.25154\n",
      "val_roc= 0.67884 val_ap= 0.72621\n",
      "Epoch: 0012 train_loss= 1.38701 time= 0.99307\n",
      "val_roc= 0.67841 val_ap= 0.72259\n",
      "Epoch: 0013 train_loss= 6.01238 time= 0.60791\n",
      "val_roc= 0.67890 val_ap= 0.72634\n",
      "Epoch: 0014 train_loss= 1.26810 time= 1.23055\n",
      "val_roc= 0.67929 val_ap= 0.72607\n",
      "Epoch: 0015 train_loss= 1.60637 time= 0.99303\n",
      "val_roc= 0.67946 val_ap= 0.72590\n",
      "Epoch: 0016 train_loss= 1.65554 time= 1.15023\n",
      "val_roc= 0.67994 val_ap= 0.72506\n",
      "Epoch: 0017 train_loss= 1.71985 time= 1.20449\n",
      "val_roc= 0.67757 val_ap= 0.71084\n",
      "Epoch: 0018 train_loss= 1.71060 time= 0.26482\n",
      "val_roc= 0.62246 val_ap= 0.60096\n",
      "Epoch: 0019 train_loss= 1.74830 time= 0.18764\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0020 train_loss= 1.76160 time= 0.20720\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0021 train_loss= 1.71506 time= 0.26917\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0022 train_loss= 1.77867 time= 0.19395\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0023 train_loss= 1.72352 time= 0.23511\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0024 train_loss= 1.73673 time= 0.18737\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0025 train_loss= 1.69916 time= 0.18321\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0026 train_loss= 1.75798 time= 0.23015\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0027 train_loss= 1.73891 time= 0.18759\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0028 train_loss= 1.74730 time= 0.21234\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0029 train_loss= 1.79446 time= 0.17330\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0030 train_loss= 1.70201 time= 0.17014\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0031 train_loss= 1.73581 time= 0.17840\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0032 train_loss= 1.70500 time= 0.18523\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0033 train_loss= 1.73935 time= 0.18880\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0034 train_loss= 1.70977 time= 0.18625\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0035 train_loss= 1.70827 time= 0.17037\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0036 train_loss= 1.75615 time= 0.17831\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0037 train_loss= 1.72851 time= 0.22047\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0038 train_loss= 1.72995 time= 0.18438\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0039 train_loss= 1.69973 time= 0.20408\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0040 train_loss= 1.73190 time= 0.18203\n",
      "val_roc= 0.61286 val_ap= 0.60035\n",
      "Epoch: 0041 train_loss= 1.77116 time= 0.19488\n",
      "val_roc= 0.63607 val_ap= 0.65357\n",
      "Epoch: 0042 train_loss= 1.72732 time= 0.20576\n",
      "val_roc= 0.66747 val_ap= 0.71096\n",
      "Epoch: 0043 train_loss= 1.70155 time= 0.20754\n",
      "val_roc= 0.67169 val_ap= 0.72152\n",
      "Epoch: 0044 train_loss= 1.73486 time= 0.19313\n",
      "val_roc= 0.67203 val_ap= 0.72317\n",
      "Epoch: 0045 train_loss= 1.67462 time= 0.21413\n",
      "val_roc= 0.67332 val_ap= 0.72424\n",
      "Epoch: 0046 train_loss= 1.57069 time= 0.17713\n",
      "val_roc= 0.67326 val_ap= 0.72427\n",
      "Epoch: 0047 train_loss= 1.22997 time= 0.18900\n",
      "val_roc= 0.67394 val_ap= 0.72446\n",
      "Epoch: 0048 train_loss= 0.80494 time= 0.16548\n",
      "val_roc= 0.67438 val_ap= 0.72500\n",
      "Epoch: 0049 train_loss= 1.48425 time= 0.17535\n",
      "val_roc= 0.67439 val_ap= 0.72549\n",
      "Epoch: 0050 train_loss= 0.82062 time= 0.17451\n",
      "val_roc= 0.67410 val_ap= 0.72571\n",
      "Epoch: 0051 train_loss= 1.15922 time= 0.19294\n",
      "val_roc= 0.67436 val_ap= 0.72624\n",
      "Epoch: 0052 train_loss= 1.38789 time= 0.17971\n",
      "val_roc= 0.67445 val_ap= 0.72624\n",
      "Epoch: 0053 train_loss= 1.48953 time= 0.17483\n",
      "val_roc= 0.67492 val_ap= 0.72667\n",
      "Epoch: 0054 train_loss= 1.47475 time= 0.18287\n",
      "val_roc= 0.67504 val_ap= 0.72674\n",
      "Epoch: 0055 train_loss= 1.48246 time= 0.21444\n",
      "val_roc= 0.67519 val_ap= 0.72688\n",
      "Epoch: 0056 train_loss= 1.45738 time= 0.19616\n",
      "val_roc= 0.67564 val_ap= 0.72692\n",
      "Epoch: 0057 train_loss= 1.28166 time= 0.20699\n",
      "val_roc= 0.67587 val_ap= 0.72693\n",
      "Epoch: 0058 train_loss= 1.09768 time= 0.20849\n",
      "val_roc= 0.67623 val_ap= 0.72684\n",
      "Epoch: 0059 train_loss= 0.83244 time= 0.18720\n",
      "val_roc= 0.67621 val_ap= 0.72691\n",
      "Epoch: 0060 train_loss= 0.79438 time= 0.18978\n",
      "val_roc= 0.67659 val_ap= 0.72706\n",
      "Epoch: 0061 train_loss= 1.15725 time= 0.19154\n",
      "val_roc= 0.67662 val_ap= 0.72715\n",
      "Epoch: 0062 train_loss= 0.74406 time= 0.20000\n",
      "val_roc= 0.67681 val_ap= 0.72738\n",
      "Epoch: 0063 train_loss= 0.73988 time= 0.23414\n",
      "val_roc= 0.67727 val_ap= 0.72774\n",
      "Epoch: 0064 train_loss= 0.78341 time= 0.18328\n",
      "val_roc= 0.67753 val_ap= 0.72800\n",
      "Epoch: 0065 train_loss= 0.83939 time= 0.21511\n",
      "val_roc= 0.67795 val_ap= 0.72836\n",
      "Epoch: 0066 train_loss= 0.85071 time= 0.18090\n",
      "val_roc= 0.67836 val_ap= 0.72856\n",
      "Epoch: 0067 train_loss= 0.81449 time= 0.22491\n",
      "val_roc= 0.67874 val_ap= 0.72877\n",
      "Epoch: 0068 train_loss= 0.76760 time= 0.19673\n",
      "val_roc= 0.67892 val_ap= 0.72896\n",
      "Epoch: 0069 train_loss= 0.73787 time= 0.18639\n",
      "val_roc= 0.67903 val_ap= 0.72886\n",
      "Epoch: 0070 train_loss= 0.73404 time= 0.25019\n",
      "val_roc= 0.67960 val_ap= 0.72924\n",
      "Epoch: 0071 train_loss= 0.73766 time= 0.18202\n",
      "val_roc= 0.67955 val_ap= 0.72912\n",
      "Epoch: 0072 train_loss= 0.75660 time= 0.23137\n",
      "val_roc= 0.67967 val_ap= 0.72918\n",
      "Epoch: 0073 train_loss= 0.78420 time= 0.21883\n",
      "val_roc= 0.67964 val_ap= 0.72915\n",
      "Epoch: 0074 train_loss= 0.77222 time= 0.20749\n",
      "val_roc= 0.67958 val_ap= 0.72919\n",
      "Epoch: 0075 train_loss= 0.74297 time= 0.21663\n",
      "val_roc= 0.67942 val_ap= 0.72912\n",
      "Epoch: 0076 train_loss= 0.73368 time= 0.19854\n",
      "val_roc= 0.67963 val_ap= 0.72948\n",
      "Epoch: 0077 train_loss= 0.73127 time= 0.23214\n",
      "val_roc= 0.67958 val_ap= 0.72964\n",
      "Epoch: 0078 train_loss= 0.73261 time= 0.20902\n",
      "val_roc= 0.67948 val_ap= 0.72949\n",
      "Epoch: 0079 train_loss= 0.73908 time= 0.19195\n",
      "val_roc= 0.67964 val_ap= 0.72950\n",
      "Epoch: 0080 train_loss= 0.74574 time= 0.22615\n",
      "val_roc= 0.67981 val_ap= 0.72969\n",
      "Epoch: 0081 train_loss= 0.74904 time= 0.18177\n",
      "val_roc= 0.68007 val_ap= 0.72988\n",
      "Epoch: 0082 train_loss= 0.74563 time= 0.19548\n",
      "val_roc= 0.68005 val_ap= 0.72982\n",
      "Epoch: 0083 train_loss= 0.74141 time= 0.25317\n",
      "val_roc= 0.68045 val_ap= 0.73012\n",
      "Epoch: 0084 train_loss= 0.73712 time= 0.18409\n",
      "val_roc= 0.68058 val_ap= 0.73021\n",
      "Epoch: 0085 train_loss= 0.73063 time= 0.24888\n",
      "val_roc= 0.68083 val_ap= 0.73041\n",
      "Epoch: 0086 train_loss= 0.73259 time= 0.17143\n",
      "val_roc= 0.68080 val_ap= 0.73052\n",
      "Epoch: 0087 train_loss= 0.73247 time= 0.19776\n",
      "val_roc= 0.68077 val_ap= 0.73057\n",
      "Epoch: 0088 train_loss= 0.73381 time= 0.16155\n",
      "val_roc= 0.68083 val_ap= 0.73074\n",
      "Epoch: 0089 train_loss= 0.73430 time= 0.18038\n",
      "val_roc= 0.68096 val_ap= 0.73092\n",
      "Epoch: 0090 train_loss= 0.73459 time= 0.16324\n",
      "val_roc= 0.68078 val_ap= 0.73093\n",
      "Epoch: 0091 train_loss= 0.73366 time= 0.18382\n",
      "val_roc= 0.68055 val_ap= 0.73079\n",
      "Epoch: 0092 train_loss= 0.73290 time= 0.24511\n",
      "val_roc= 0.68033 val_ap= 0.73078\n",
      "Epoch: 0093 train_loss= 0.72985 time= 0.17223\n",
      "val_roc= 0.68052 val_ap= 0.73099\n",
      "Epoch: 0094 train_loss= 0.72714 time= 0.24395\n",
      "val_roc= 0.68048 val_ap= 0.73099\n",
      "Epoch: 0095 train_loss= 0.72429 time= 0.17798\n",
      "val_roc= 0.68038 val_ap= 0.73094\n",
      "Epoch: 0096 train_loss= 0.72326 time= 0.20374\n",
      "val_roc= 0.68035 val_ap= 0.73096\n",
      "Epoch: 0097 train_loss= 0.72216 time= 0.24701\n",
      "val_roc= 0.68045 val_ap= 0.73109\n",
      "Epoch: 0098 train_loss= 0.72442 time= 0.17596\n",
      "val_roc= 0.68041 val_ap= 0.73115\n",
      "Epoch: 0099 train_loss= 0.72587 time= 0.20324\n",
      "val_roc= 0.68049 val_ap= 0.73123\n",
      "Epoch: 0100 train_loss= 0.72557 time= 0.17519\n",
      "val_roc= 0.68071 val_ap= 0.73138\n",
      "Epoch: 0101 train_loss= 0.71978 time= 0.17947\n",
      "val_roc= 0.68084 val_ap= 0.73141\n",
      "Epoch: 0102 train_loss= 0.71922 time= 0.23759\n",
      "val_roc= 0.68104 val_ap= 0.73151\n",
      "Epoch: 0103 train_loss= 0.72162 time= 0.17625\n",
      "val_roc= 0.68132 val_ap= 0.73170\n",
      "Epoch: 0104 train_loss= 0.72139 time= 0.22399\n",
      "val_roc= 0.68130 val_ap= 0.73162\n",
      "Epoch: 0105 train_loss= 0.72056 time= 0.16521\n",
      "val_roc= 0.68155 val_ap= 0.73174\n",
      "Epoch: 0106 train_loss= 0.72192 time= 0.19243\n",
      "val_roc= 0.68149 val_ap= 0.73170\n",
      "Epoch: 0107 train_loss= 0.72159 time= 0.20210\n",
      "val_roc= 0.68158 val_ap= 0.73167\n",
      "Epoch: 0108 train_loss= 0.71774 time= 0.20608\n",
      "val_roc= 0.68179 val_ap= 0.73170\n",
      "Epoch: 0109 train_loss= 0.72001 time= 0.21903\n",
      "val_roc= 0.68197 val_ap= 0.73171\n",
      "Epoch: 0110 train_loss= 0.72094 time= 0.19168\n",
      "val_roc= 0.68230 val_ap= 0.73187\n",
      "Epoch: 0111 train_loss= 0.72109 time= 0.19344\n",
      "val_roc= 0.68226 val_ap= 0.73181\n",
      "Epoch: 0112 train_loss= 0.71967 time= 0.23038\n",
      "val_roc= 0.68249 val_ap= 0.73190\n",
      "Epoch: 0113 train_loss= 0.72057 time= 0.17210\n",
      "val_roc= 0.68305 val_ap= 0.73210\n",
      "Epoch: 0114 train_loss= 0.72006 time= 0.20258\n",
      "val_roc= 0.68341 val_ap= 0.73236\n",
      "Epoch: 0115 train_loss= 0.71766 time= 0.19673\n",
      "val_roc= 0.68360 val_ap= 0.73245\n",
      "Epoch: 0116 train_loss= 0.72052 time= 0.17885\n",
      "val_roc= 0.68418 val_ap= 0.73278\n",
      "Epoch: 0117 train_loss= 0.72035 time= 0.23408\n",
      "val_roc= 0.68453 val_ap= 0.73291\n",
      "Epoch: 0118 train_loss= 0.71945 time= 0.21775\n",
      "val_roc= 0.68480 val_ap= 0.73304\n",
      "Epoch: 0119 train_loss= 0.71828 time= 0.21293\n",
      "val_roc= 0.68529 val_ap= 0.73330\n",
      "Epoch: 0120 train_loss= 0.72031 time= 0.22557\n",
      "val_roc= 0.68551 val_ap= 0.73317\n",
      "Epoch: 0121 train_loss= 0.72055 time= 0.17658\n",
      "val_roc= 0.68570 val_ap= 0.73330\n",
      "Epoch: 0122 train_loss= 0.71993 time= 0.22185\n",
      "val_roc= 0.68600 val_ap= 0.73346\n",
      "Epoch: 0123 train_loss= 0.72040 time= 0.20229\n",
      "val_roc= 0.68594 val_ap= 0.73345\n",
      "Epoch: 0124 train_loss= 0.71774 time= 0.17805\n",
      "val_roc= 0.68623 val_ap= 0.73353\n",
      "Epoch: 0125 train_loss= 0.71999 time= 0.23369\n",
      "val_roc= 0.68642 val_ap= 0.73362\n",
      "Epoch: 0126 train_loss= 0.71677 time= 0.17252\n",
      "val_roc= 0.68672 val_ap= 0.73377\n",
      "Epoch: 0127 train_loss= 0.71756 time= 0.19441\n",
      "val_roc= 0.68706 val_ap= 0.73396\n",
      "Epoch: 0128 train_loss= 0.71933 time= 0.22031\n",
      "val_roc= 0.68732 val_ap= 0.73399\n",
      "Epoch: 0129 train_loss= 0.71810 time= 0.16861\n",
      "val_roc= 0.68758 val_ap= 0.73421\n",
      "Epoch: 0130 train_loss= 0.71770 time= 0.22985\n",
      "val_roc= 0.68794 val_ap= 0.73445\n",
      "Epoch: 0131 train_loss= 0.72019 time= 0.17047\n",
      "val_roc= 0.68833 val_ap= 0.73454\n",
      "Epoch: 0132 train_loss= 0.71772 time= 0.21321\n",
      "val_roc= 0.68826 val_ap= 0.73455\n",
      "Epoch: 0133 train_loss= 0.71852 time= 0.25292\n",
      "val_roc= 0.68818 val_ap= 0.73460\n",
      "Epoch: 0134 train_loss= 0.71697 time= 0.18477\n",
      "val_roc= 0.68817 val_ap= 0.73455\n",
      "Epoch: 0135 train_loss= 0.71749 time= 0.23268\n",
      "val_roc= 0.68826 val_ap= 0.73461\n",
      "Epoch: 0136 train_loss= 0.71916 time= 0.23589\n",
      "val_roc= 0.68846 val_ap= 0.73489\n",
      "Epoch: 0137 train_loss= 0.71743 time= 0.20377\n",
      "val_roc= 0.68850 val_ap= 0.73496\n",
      "Epoch: 0138 train_loss= 0.71713 time= 0.19436\n",
      "val_roc= 0.68870 val_ap= 0.73509\n",
      "Epoch: 0139 train_loss= 0.71756 time= 0.23839\n",
      "val_roc= 0.68885 val_ap= 0.73536\n",
      "Epoch: 0140 train_loss= 0.71616 time= 0.18436\n",
      "val_roc= 0.68905 val_ap= 0.73556\n",
      "Epoch: 0141 train_loss= 0.71491 time= 0.23803\n",
      "val_roc= 0.68902 val_ap= 0.73552\n",
      "Epoch: 0142 train_loss= 0.71608 time= 0.18316\n",
      "val_roc= 0.68925 val_ap= 0.73577\n",
      "Epoch: 0143 train_loss= 0.71899 time= 0.20013\n",
      "val_roc= 0.68915 val_ap= 0.73561\n",
      "Epoch: 0144 train_loss= 0.71820 time= 0.24504\n",
      "val_roc= 0.68911 val_ap= 0.73556\n",
      "Epoch: 0145 train_loss= 0.71594 time= 0.17710\n",
      "val_roc= 0.68899 val_ap= 0.73541\n",
      "Epoch: 0146 train_loss= 0.71456 time= 0.23380\n",
      "val_roc= 0.68904 val_ap= 0.73535\n",
      "Epoch: 0147 train_loss= 0.71758 time= 0.21620\n",
      "val_roc= 0.68896 val_ap= 0.73527\n",
      "Epoch: 0148 train_loss= 0.71590 time= 0.19002\n",
      "val_roc= 0.68901 val_ap= 0.73516\n",
      "Epoch: 0149 train_loss= 0.71883 time= 0.24305\n",
      "val_roc= 0.68892 val_ap= 0.73500\n",
      "Epoch: 0150 train_loss= 0.71713 time= 0.18807\n",
      "val_roc= 0.68894 val_ap= 0.73508\n",
      "Epoch: 0151 train_loss= 0.71976 time= 0.19209\n",
      "val_roc= 0.68904 val_ap= 0.73505\n",
      "Epoch: 0152 train_loss= 0.71679 time= 0.24128\n",
      "val_roc= 0.68921 val_ap= 0.73529\n",
      "Epoch: 0153 train_loss= 0.71679 time= 0.17527\n",
      "val_roc= 0.68956 val_ap= 0.73563\n",
      "Epoch: 0154 train_loss= 0.71755 time= 0.22906\n",
      "val_roc= 0.68976 val_ap= 0.73567\n",
      "Epoch: 0155 train_loss= 0.71785 time= 0.25338\n",
      "val_roc= 0.68960 val_ap= 0.73548\n",
      "Epoch: 0156 train_loss= 0.71448 time= 0.17854\n",
      "val_roc= 0.68967 val_ap= 0.73539\n",
      "Epoch: 0157 train_loss= 0.71573 time= 0.22085\n",
      "val_roc= 0.68946 val_ap= 0.73508\n",
      "Epoch: 0158 train_loss= 0.71672 time= 0.23622\n",
      "val_roc= 0.68934 val_ap= 0.73493\n",
      "Epoch: 0159 train_loss= 0.71667 time= 0.21032\n",
      "val_roc= 0.68938 val_ap= 0.73496\n",
      "Epoch: 0160 train_loss= 0.71490 time= 0.23492\n",
      "val_roc= 0.68943 val_ap= 0.73509\n",
      "Epoch: 0161 train_loss= 0.71349 time= 0.19572\n",
      "val_roc= 0.68938 val_ap= 0.73507\n",
      "Epoch: 0162 train_loss= 0.71445 time= 0.20911\n",
      "val_roc= 0.68966 val_ap= 0.73515\n",
      "Epoch: 0163 train_loss= 0.71394 time= 0.21849\n",
      "val_roc= 0.68933 val_ap= 0.73496\n",
      "Epoch: 0164 train_loss= 0.71538 time= 0.17009\n",
      "val_roc= 0.68943 val_ap= 0.73507\n",
      "Epoch: 0165 train_loss= 0.71495 time= 0.18739\n",
      "val_roc= 0.68967 val_ap= 0.73506\n",
      "Epoch: 0166 train_loss= 0.71577 time= 0.22018\n",
      "val_roc= 0.68963 val_ap= 0.73502\n",
      "Epoch: 0167 train_loss= 0.71436 time= 0.16390\n",
      "val_roc= 0.68979 val_ap= 0.73519\n",
      "Epoch: 0168 train_loss= 0.71636 time= 0.21540\n",
      "val_roc= 0.69019 val_ap= 0.73547\n",
      "Epoch: 0169 train_loss= 0.71435 time= 0.21027\n",
      "val_roc= 0.69021 val_ap= 0.73544\n",
      "Epoch: 0170 train_loss= 0.71483 time= 0.17861\n",
      "val_roc= 0.69007 val_ap= 0.73529\n",
      "Epoch: 0171 train_loss= 0.71527 time= 0.21306\n",
      "val_roc= 0.68988 val_ap= 0.73521\n",
      "Epoch: 0172 train_loss= 0.71519 time= 0.17429\n",
      "val_roc= 0.68950 val_ap= 0.73511\n",
      "Epoch: 0173 train_loss= 0.71325 time= 0.22993\n",
      "val_roc= 0.68963 val_ap= 0.73521\n",
      "Epoch: 0174 train_loss= 0.71373 time= 0.21943\n",
      "val_roc= 0.68964 val_ap= 0.73520\n",
      "Epoch: 0175 train_loss= 0.71528 time= 0.17263\n",
      "val_roc= 0.68928 val_ap= 0.73494\n",
      "Epoch: 0176 train_loss= 0.71533 time= 0.19662\n",
      "val_roc= 0.68943 val_ap= 0.73512\n",
      "Epoch: 0177 train_loss= 0.71488 time= 0.22379\n",
      "val_roc= 0.68950 val_ap= 0.73516\n",
      "Epoch: 0178 train_loss= 0.71341 time= 0.18974\n",
      "val_roc= 0.68905 val_ap= 0.73488\n",
      "Epoch: 0179 train_loss= 0.71415 time= 0.23637\n",
      "val_roc= 0.68881 val_ap= 0.73474\n",
      "Epoch: 0180 train_loss= 0.71396 time= 0.20042\n",
      "val_roc= 0.68872 val_ap= 0.73463\n",
      "Epoch: 0181 train_loss= 0.71635 time= 0.18028\n",
      "val_roc= 0.68874 val_ap= 0.73461\n",
      "Epoch: 0182 train_loss= 0.71327 time= 0.24500\n",
      "val_roc= 0.68904 val_ap= 0.73491\n",
      "Epoch: 0183 train_loss= 0.71335 time= 0.16900\n",
      "val_roc= 0.68894 val_ap= 0.73486\n",
      "Epoch: 0184 train_loss= 0.71591 time= 0.19269\n",
      "val_roc= 0.68875 val_ap= 0.73474\n",
      "Epoch: 0185 train_loss= 0.71266 time= 0.21570\n",
      "val_roc= 0.68837 val_ap= 0.73449\n",
      "Epoch: 0186 train_loss= 0.71240 time= 0.18421\n",
      "val_roc= 0.68808 val_ap= 0.73434\n",
      "Epoch: 0187 train_loss= 0.71367 time= 0.22271\n",
      "val_roc= 0.68820 val_ap= 0.73449\n",
      "Epoch: 0188 train_loss= 0.71493 time= 0.17484\n",
      "val_roc= 0.68813 val_ap= 0.73449\n",
      "Epoch: 0189 train_loss= 0.71279 time= 0.20070\n",
      "val_roc= 0.68791 val_ap= 0.73420\n",
      "Epoch: 0190 train_loss= 0.71130 time= 0.19623\n",
      "val_roc= 0.68779 val_ap= 0.73419\n",
      "Epoch: 0191 train_loss= 0.71274 time= 0.17415\n",
      "val_roc= 0.68761 val_ap= 0.73397\n",
      "Epoch: 0192 train_loss= 0.71296 time= 0.24679\n",
      "val_roc= 0.68765 val_ap= 0.73403\n",
      "Epoch: 0193 train_loss= 0.71523 time= 0.22524\n",
      "val_roc= 0.68761 val_ap= 0.73397\n",
      "Epoch: 0194 train_loss= 0.71254 time= 0.16794\n",
      "val_roc= 0.68746 val_ap= 0.73375\n",
      "Epoch: 0195 train_loss= 0.71360 time= 0.21244\n",
      "val_roc= 0.68733 val_ap= 0.73370\n",
      "Epoch: 0196 train_loss= 0.71254 time= 0.18118\n",
      "val_roc= 0.68697 val_ap= 0.73357\n",
      "Epoch: 0197 train_loss= 0.71479 time= 0.17384\n",
      "val_roc= 0.68668 val_ap= 0.73359\n",
      "Epoch: 0198 train_loss= 0.71074 time= 0.20850\n",
      "val_roc= 0.68619 val_ap= 0.73354\n",
      "Epoch: 0199 train_loss= 0.71534 time= 0.17134\n",
      "val_roc= 0.68625 val_ap= 0.73365\n",
      "Epoch: 0200 train_loss= 0.71139 time= 0.21878\n",
      "val_roc= 0.68594 val_ap= 0.73354\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 1.74646 time= 2.00905\n",
      "val_roc= 0.63076 val_ap= 0.66608\n",
      "Epoch: 0002 train_loss= 1.65733 time= 0.22024\n",
      "val_roc= 0.49810 val_ap= 0.49905\n",
      "Epoch: 0003 train_loss= 53585224138752.00000 time= 0.19688\n",
      "val_roc= 0.64471 val_ap= 0.69339\n",
      "Epoch: 0004 train_loss= 1.46131 time= 0.23822\n",
      "val_roc= 0.64833 val_ap= 0.69593\n",
      "Epoch: 0005 train_loss= 1.69047 time= 0.18057\n",
      "val_roc= 0.65443 val_ap= 0.69800\n",
      "Epoch: 0006 train_loss= 1.72712 time= 0.20805\n",
      "val_roc= 0.65850 val_ap= 0.69837\n",
      "Epoch: 0007 train_loss= 1.72285 time= 0.23755\n",
      "val_roc= 0.65830 val_ap= 0.68932\n",
      "Epoch: 0008 train_loss= 1.76117 time= 0.16014\n",
      "val_roc= 0.66690 val_ap= 0.67188\n",
      "Epoch: 0009 train_loss= 1.69237 time= 0.24242\n",
      "val_roc= 0.64464 val_ap= 0.64024\n",
      "Epoch: 0010 train_loss= 1.71582 time= 0.17526\n",
      "val_roc= 0.62086 val_ap= 0.61044\n",
      "Epoch: 0011 train_loss= 1.74263 time= 0.19522\n",
      "val_roc= 0.62379 val_ap= 0.61479\n",
      "Epoch: 0012 train_loss= 1.74589 time= 0.24602\n",
      "val_roc= 0.65204 val_ap= 0.64520\n",
      "Epoch: 0013 train_loss= 1.74052 time= 0.20382\n",
      "val_roc= 0.65849 val_ap= 0.67229\n",
      "Epoch: 0014 train_loss= 1.69924 time= 0.21842\n",
      "val_roc= 0.65834 val_ap= 0.68700\n",
      "Epoch: 0015 train_loss= 1.76519 time= 0.23550\n",
      "val_roc= 0.66033 val_ap= 0.70165\n",
      "Epoch: 0016 train_loss= 1.72687 time= 0.19145\n",
      "val_roc= 0.65953 val_ap= 0.70589\n",
      "Epoch: 0017 train_loss= 1.69730 time= 0.22755\n",
      "val_roc= 0.65973 val_ap= 0.70800\n",
      "Epoch: 0018 train_loss= 1.71381 time= 0.21299\n",
      "val_roc= 0.65938 val_ap= 0.70851\n",
      "Epoch: 0019 train_loss= 1.72117 time= 0.18852\n",
      "val_roc= 0.65915 val_ap= 0.70853\n",
      "Epoch: 0020 train_loss= 1.75163 time= 0.23418\n",
      "val_roc= 0.65753 val_ap= 0.70704\n",
      "Epoch: 0021 train_loss= 1.70216 time= 0.19694\n",
      "val_roc= 0.65678 val_ap= 0.70644\n",
      "Epoch: 0022 train_loss= 1.64261 time= 0.20245\n",
      "val_roc= 0.65620 val_ap= 0.70524\n",
      "Epoch: 0023 train_loss= 1.60750 time= 0.18428\n",
      "val_roc= 0.65493 val_ap= 0.70378\n",
      "Epoch: 0024 train_loss= 1.41824 time= 0.16754\n",
      "val_roc= 0.65383 val_ap= 0.70283\n",
      "Epoch: 0025 train_loss= 1.19796 time= 0.21723\n",
      "val_roc= 0.65343 val_ap= 0.70247\n",
      "Epoch: 0026 train_loss= 1.91347 time= 0.19695\n",
      "val_roc= 0.65409 val_ap= 0.70307\n",
      "Epoch: 0027 train_loss= 1.16852 time= 0.16624\n",
      "val_roc= 0.65451 val_ap= 0.70353\n",
      "Epoch: 0028 train_loss= 1.25793 time= 0.22268\n",
      "val_roc= 0.65487 val_ap= 0.70392\n",
      "Epoch: 0029 train_loss= 1.40216 time= 0.19807\n",
      "val_roc= 0.65531 val_ap= 0.70426\n",
      "Epoch: 0030 train_loss= 1.46421 time= 0.18473\n",
      "val_roc= 0.65561 val_ap= 0.70464\n",
      "Epoch: 0031 train_loss= 1.48024 time= 0.25602\n",
      "val_roc= 0.65583 val_ap= 0.70520\n",
      "Epoch: 0032 train_loss= 1.51143 time= 0.16986\n",
      "val_roc= 0.65595 val_ap= 0.70527\n",
      "Epoch: 0033 train_loss= 1.53099 time= 0.20610\n",
      "val_roc= 0.65606 val_ap= 0.70537\n",
      "Epoch: 0034 train_loss= 1.58778 time= 0.20692\n",
      "val_roc= 0.65603 val_ap= 0.70540\n",
      "Epoch: 0035 train_loss= 1.58151 time= 0.20445\n",
      "val_roc= 0.65595 val_ap= 0.70519\n",
      "Epoch: 0036 train_loss= 1.54949 time= 0.21641\n",
      "val_roc= 0.65587 val_ap= 0.70518\n",
      "Epoch: 0037 train_loss= 1.53015 time= 0.22228\n",
      "val_roc= 0.65590 val_ap= 0.70505\n",
      "Epoch: 0038 train_loss= 1.45884 time= 0.19841\n",
      "val_roc= 0.65580 val_ap= 0.70504\n",
      "Epoch: 0039 train_loss= 1.46847 time= 0.24261\n",
      "val_roc= 0.65561 val_ap= 0.70486\n",
      "Epoch: 0040 train_loss= 1.38528 time= 0.19331\n",
      "val_roc= 0.65538 val_ap= 0.70462\n",
      "Epoch: 0041 train_loss= 1.35358 time= 0.20037\n",
      "val_roc= 0.65499 val_ap= 0.70418\n",
      "Epoch: 0042 train_loss= 1.19120 time= 0.24233\n",
      "val_roc= 0.65472 val_ap= 0.70402\n",
      "Epoch: 0043 train_loss= 1.06978 time= 0.16528\n",
      "val_roc= 0.65461 val_ap= 0.70390\n",
      "Epoch: 0044 train_loss= 0.92172 time= 0.22700\n",
      "val_roc= 0.65453 val_ap= 0.70346\n",
      "Epoch: 0045 train_loss= 0.85966 time= 0.20572\n",
      "val_roc= 0.65427 val_ap= 0.70320\n",
      "Epoch: 0046 train_loss= 1.06156 time= 0.20048\n",
      "val_roc= 0.65430 val_ap= 0.70304\n",
      "Epoch: 0047 train_loss= 1.04848 time= 0.24683\n",
      "val_roc= 0.65456 val_ap= 0.70320\n",
      "Epoch: 0048 train_loss= 0.84974 time= 0.19509\n",
      "val_roc= 0.65490 val_ap= 0.70378\n",
      "Epoch: 0049 train_loss= 0.81209 time= 0.20470\n",
      "val_roc= 0.65519 val_ap= 0.70415\n",
      "Epoch: 0050 train_loss= 0.85119 time= 0.24763\n",
      "val_roc= 0.65538 val_ap= 0.70431\n",
      "Epoch: 0051 train_loss= 0.88348 time= 0.22178\n",
      "val_roc= 0.65552 val_ap= 0.70443\n",
      "Epoch: 0052 train_loss= 0.90230 time= 0.20588\n",
      "val_roc= 0.65557 val_ap= 0.70442\n",
      "Epoch: 0053 train_loss= 0.87803 time= 0.19565\n",
      "val_roc= 0.65550 val_ap= 0.70431\n",
      "Epoch: 0054 train_loss= 0.84468 time= 0.22154\n",
      "val_roc= 0.65534 val_ap= 0.70414\n",
      "Epoch: 0055 train_loss= 0.80571 time= 0.18565\n",
      "val_roc= 0.65516 val_ap= 0.70397\n",
      "Epoch: 0056 train_loss= 0.76846 time= 0.23243\n",
      "val_roc= 0.65495 val_ap= 0.70369\n",
      "Epoch: 0057 train_loss= 0.74232 time= 0.24003\n",
      "val_roc= 0.65492 val_ap= 0.70372\n",
      "Epoch: 0058 train_loss= 0.73894 time= 0.19356\n",
      "val_roc= 0.65492 val_ap= 0.70369\n",
      "Epoch: 0059 train_loss= 0.74968 time= 0.21568\n",
      "val_roc= 0.65480 val_ap= 0.70367\n",
      "Epoch: 0060 train_loss= 0.76348 time= 0.18937\n",
      "val_roc= 0.65473 val_ap= 0.70362\n",
      "Epoch: 0061 train_loss= 0.75471 time= 0.19440\n",
      "val_roc= 0.65490 val_ap= 0.70373\n",
      "Epoch: 0062 train_loss= 0.73966 time= 0.22484\n",
      "val_roc= 0.65500 val_ap= 0.70384\n",
      "Epoch: 0063 train_loss= 0.73070 time= 0.18394\n",
      "val_roc= 0.65505 val_ap= 0.70385\n",
      "Epoch: 0064 train_loss= 0.72650 time= 0.19342\n",
      "val_roc= 0.65513 val_ap= 0.70404\n",
      "Epoch: 0065 train_loss= 0.72955 time= 0.23308\n",
      "val_roc= 0.65524 val_ap= 0.70412\n",
      "Epoch: 0066 train_loss= 0.73382 time= 0.17791\n",
      "val_roc= 0.65526 val_ap= 0.70411\n",
      "Epoch: 0067 train_loss= 0.73560 time= 0.21272\n",
      "val_roc= 0.65544 val_ap= 0.70432\n",
      "Epoch: 0068 train_loss= 0.73550 time= 0.24923\n",
      "val_roc= 0.65555 val_ap= 0.70438\n",
      "Epoch: 0069 train_loss= 0.73713 time= 0.17872\n",
      "val_roc= 0.65567 val_ap= 0.70446\n",
      "Epoch: 0070 train_loss= 0.73621 time= 0.25658\n",
      "val_roc= 0.65577 val_ap= 0.70462\n",
      "Epoch: 0071 train_loss= 0.73681 time= 0.20799\n",
      "val_roc= 0.65589 val_ap= 0.70472\n",
      "Epoch: 0072 train_loss= 0.73907 time= 0.18573\n",
      "val_roc= 0.65592 val_ap= 0.70473\n",
      "Epoch: 0073 train_loss= 0.73538 time= 0.23481\n",
      "val_roc= 0.65585 val_ap= 0.70466\n",
      "Epoch: 0074 train_loss= 0.73446 time= 0.17285\n",
      "val_roc= 0.65594 val_ap= 0.70473\n",
      "Epoch: 0075 train_loss= 0.73099 time= 0.22897\n",
      "val_roc= 0.65593 val_ap= 0.70474\n",
      "Epoch: 0076 train_loss= 0.73208 time= 0.25768\n",
      "val_roc= 0.65594 val_ap= 0.70476\n",
      "Epoch: 0077 train_loss= 0.72818 time= 0.20504\n",
      "val_roc= 0.65602 val_ap= 0.70481\n",
      "Epoch: 0078 train_loss= 0.72818 time= 0.20065\n",
      "val_roc= 0.65615 val_ap= 0.70487\n",
      "Epoch: 0079 train_loss= 0.72737 time= 0.24372\n",
      "val_roc= 0.65626 val_ap= 0.70504\n",
      "Epoch: 0080 train_loss= 0.72734 time= 0.19130\n",
      "val_roc= 0.65635 val_ap= 0.70511\n",
      "Epoch: 0081 train_loss= 0.72889 time= 0.24098\n",
      "val_roc= 0.65642 val_ap= 0.70516\n",
      "Epoch: 0082 train_loss= 0.72982 time= 0.20323\n",
      "val_roc= 0.65657 val_ap= 0.70528\n",
      "Epoch: 0083 train_loss= 0.72457 time= 0.19671\n",
      "val_roc= 0.65667 val_ap= 0.70542\n",
      "Epoch: 0084 train_loss= 0.72702 time= 0.25018\n",
      "val_roc= 0.65677 val_ap= 0.70552\n",
      "Epoch: 0085 train_loss= 0.72637 time= 0.19576\n",
      "val_roc= 0.65683 val_ap= 0.70563\n",
      "Epoch: 0086 train_loss= 0.72457 time= 0.20975\n",
      "val_roc= 0.65688 val_ap= 0.70568\n",
      "Epoch: 0087 train_loss= 0.72488 time= 0.26140\n",
      "val_roc= 0.65699 val_ap= 0.70583\n",
      "Epoch: 0088 train_loss= 0.72638 time= 0.18442\n",
      "val_roc= 0.65694 val_ap= 0.70580\n",
      "Epoch: 0089 train_loss= 0.72580 time= 0.19287\n",
      "val_roc= 0.65712 val_ap= 0.70598\n",
      "Epoch: 0090 train_loss= 0.72681 time= 0.24195\n",
      "val_roc= 0.65717 val_ap= 0.70589\n",
      "Epoch: 0091 train_loss= 0.72372 time= 0.17059\n",
      "val_roc= 0.65716 val_ap= 0.70583\n",
      "Epoch: 0092 train_loss= 0.72880 time= 0.20665\n",
      "val_roc= 0.65727 val_ap= 0.70594\n",
      "Epoch: 0093 train_loss= 0.72396 time= 0.20961\n",
      "val_roc= 0.65726 val_ap= 0.70590\n",
      "Epoch: 0094 train_loss= 0.72302 time= 0.17965\n",
      "val_roc= 0.65723 val_ap= 0.70588\n",
      "Epoch: 0095 train_loss= 0.72261 time= 0.20806\n",
      "val_roc= 0.65719 val_ap= 0.70587\n",
      "Epoch: 0096 train_loss= 0.72308 time= 0.19436\n",
      "val_roc= 0.65727 val_ap= 0.70592\n",
      "Epoch: 0097 train_loss= 0.72339 time= 0.20051\n",
      "val_roc= 0.65727 val_ap= 0.70587\n",
      "Epoch: 0098 train_loss= 0.72458 time= 0.23790\n",
      "val_roc= 0.65742 val_ap= 0.70596\n",
      "Epoch: 0099 train_loss= 0.72218 time= 0.19309\n",
      "val_roc= 0.65748 val_ap= 0.70600\n",
      "Epoch: 0100 train_loss= 0.72103 time= 0.17053\n",
      "val_roc= 0.65752 val_ap= 0.70605\n",
      "Epoch: 0101 train_loss= 0.72231 time= 0.21993\n",
      "val_roc= 0.65756 val_ap= 0.70609\n",
      "Epoch: 0102 train_loss= 0.72427 time= 0.17975\n",
      "val_roc= 0.65761 val_ap= 0.70611\n",
      "Epoch: 0103 train_loss= 0.72252 time= 0.18938\n",
      "val_roc= 0.65772 val_ap= 0.70618\n",
      "Epoch: 0104 train_loss= 0.72228 time= 0.25296\n",
      "val_roc= 0.65765 val_ap= 0.70613\n",
      "Epoch: 0105 train_loss= 0.72449 time= 0.16009\n",
      "val_roc= 0.65775 val_ap= 0.70621\n",
      "Epoch: 0106 train_loss= 0.72361 time= 0.21024\n",
      "val_roc= 0.65784 val_ap= 0.70636\n",
      "Epoch: 0107 train_loss= 0.72097 time= 0.21335\n",
      "val_roc= 0.65788 val_ap= 0.70662\n",
      "Epoch: 0108 train_loss= 0.72119 time= 0.18519\n",
      "val_roc= 0.65795 val_ap= 0.70667\n",
      "Epoch: 0109 train_loss= 0.72235 time= 0.23864\n",
      "val_roc= 0.65811 val_ap= 0.70678\n",
      "Epoch: 0110 train_loss= 0.72168 time= 0.20217\n",
      "val_roc= 0.65820 val_ap= 0.70696\n",
      "Epoch: 0111 train_loss= 0.72400 time= 0.21158\n",
      "val_roc= 0.65833 val_ap= 0.70710\n",
      "Epoch: 0112 train_loss= 0.72300 time= 0.22302\n",
      "val_roc= 0.65850 val_ap= 0.70727\n",
      "Epoch: 0113 train_loss= 0.72077 time= 0.19310\n",
      "val_roc= 0.65862 val_ap= 0.70735\n",
      "Epoch: 0114 train_loss= 0.72256 time= 0.21962\n",
      "val_roc= 0.65875 val_ap= 0.70749\n",
      "Epoch: 0115 train_loss= 0.72253 time= 0.18448\n",
      "val_roc= 0.65866 val_ap= 0.70738\n",
      "Epoch: 0116 train_loss= 0.72208 time= 0.19873\n",
      "val_roc= 0.65885 val_ap= 0.70747\n",
      "Epoch: 0117 train_loss= 0.72168 time= 0.23353\n",
      "val_roc= 0.65898 val_ap= 0.70760\n",
      "Epoch: 0118 train_loss= 0.72140 time= 0.18417\n",
      "val_roc= 0.65911 val_ap= 0.70766\n",
      "Epoch: 0119 train_loss= 0.72625 time= 0.19814\n",
      "val_roc= 0.65928 val_ap= 0.70779\n",
      "Epoch: 0120 train_loss= 0.71842 time= 0.23634\n",
      "val_roc= 0.65930 val_ap= 0.70792\n",
      "Epoch: 0121 train_loss= 0.72066 time= 0.21743\n",
      "val_roc= 0.65937 val_ap= 0.70801\n",
      "Epoch: 0122 train_loss= 0.71952 time= 0.17304\n",
      "val_roc= 0.65938 val_ap= 0.70810\n",
      "Epoch: 0123 train_loss= 0.72143 time= 0.21563\n",
      "val_roc= 0.65956 val_ap= 0.70821\n",
      "Epoch: 0124 train_loss= 0.72189 time= 0.18101\n",
      "val_roc= 0.65972 val_ap= 0.70831\n",
      "Epoch: 0125 train_loss= 0.72280 time= 0.18420\n",
      "val_roc= 0.65980 val_ap= 0.70838\n",
      "Epoch: 0126 train_loss= 0.72167 time= 0.23518\n",
      "val_roc= 0.65988 val_ap= 0.70855\n",
      "Epoch: 0127 train_loss= 0.72177 time= 0.17573\n",
      "val_roc= 0.65999 val_ap= 0.70862\n",
      "Epoch: 0128 train_loss= 0.72437 time= 0.20190\n",
      "val_roc= 0.66005 val_ap= 0.70869\n",
      "Epoch: 0129 train_loss= 0.71941 time= 0.20158\n",
      "val_roc= 0.66021 val_ap= 0.70880\n",
      "Epoch: 0130 train_loss= 0.72140 time= 0.17678\n",
      "val_roc= 0.66045 val_ap= 0.70899\n",
      "Epoch: 0131 train_loss= 0.72016 time= 0.22006\n",
      "val_roc= 0.66058 val_ap= 0.70905\n",
      "Epoch: 0132 train_loss= 0.72071 time= 0.17530\n",
      "val_roc= 0.66070 val_ap= 0.70911\n",
      "Epoch: 0133 train_loss= 0.71960 time= 0.22215\n",
      "val_roc= 0.66073 val_ap= 0.70914\n",
      "Epoch: 0134 train_loss= 0.72250 time= 0.18223\n",
      "val_roc= 0.66086 val_ap= 0.70925\n",
      "Epoch: 0135 train_loss= 0.72193 time= 0.16236\n",
      "val_roc= 0.66103 val_ap= 0.70943\n",
      "Epoch: 0136 train_loss= 0.72212 time= 0.21163\n",
      "val_roc= 0.66109 val_ap= 0.70946\n",
      "Epoch: 0137 train_loss= 0.71879 time= 0.18335\n",
      "val_roc= 0.66109 val_ap= 0.70946\n",
      "Epoch: 0138 train_loss= 0.72123 time= 0.20192\n",
      "val_roc= 0.66113 val_ap= 0.70948\n",
      "Epoch: 0139 train_loss= 0.72102 time= 0.23129\n",
      "val_roc= 0.66112 val_ap= 0.70946\n",
      "Epoch: 0140 train_loss= 0.72131 time= 0.21251\n",
      "val_roc= 0.66129 val_ap= 0.70954\n",
      "Epoch: 0141 train_loss= 0.71949 time= 0.19330\n",
      "val_roc= 0.66132 val_ap= 0.70958\n",
      "Epoch: 0142 train_loss= 0.71946 time= 0.19930\n",
      "val_roc= 0.66135 val_ap= 0.70966\n",
      "Epoch: 0143 train_loss= 0.72230 time= 0.21246\n",
      "val_roc= 0.66144 val_ap= 0.70976\n",
      "Epoch: 0144 train_loss= 0.71923 time= 0.19099\n",
      "val_roc= 0.66150 val_ap= 0.70980\n",
      "Epoch: 0145 train_loss= 0.72042 time= 0.21327\n",
      "val_roc= 0.66148 val_ap= 0.70978\n",
      "Epoch: 0146 train_loss= 0.71910 time= 0.16433\n",
      "val_roc= 0.66158 val_ap= 0.70994\n",
      "Epoch: 0147 train_loss= 0.71858 time= 0.17835\n",
      "val_roc= 0.66165 val_ap= 0.70998\n",
      "Epoch: 0148 train_loss= 0.72259 time= 0.16151\n",
      "val_roc= 0.66174 val_ap= 0.71005\n",
      "Epoch: 0149 train_loss= 0.72097 time= 0.16879\n",
      "val_roc= 0.66183 val_ap= 0.71009\n",
      "Epoch: 0150 train_loss= 0.71878 time= 0.17022\n",
      "val_roc= 0.66194 val_ap= 0.71016\n",
      "Epoch: 0151 train_loss= 0.72223 time= 0.16619\n",
      "val_roc= 0.66190 val_ap= 0.71019\n",
      "Epoch: 0152 train_loss= 0.71917 time= 0.17633\n",
      "val_roc= 0.66193 val_ap= 0.71022\n",
      "Epoch: 0153 train_loss= 0.72214 time= 0.19369\n",
      "val_roc= 0.66210 val_ap= 0.71033\n",
      "Epoch: 0154 train_loss= 0.71869 time= 0.16387\n",
      "val_roc= 0.66209 val_ap= 0.71035\n",
      "Epoch: 0155 train_loss= 0.72034 time= 0.16689\n",
      "val_roc= 0.66209 val_ap= 0.71033\n",
      "Epoch: 0156 train_loss= 0.71786 time= 0.16699\n",
      "val_roc= 0.66223 val_ap= 0.71044\n",
      "Epoch: 0157 train_loss= 0.71778 time= 0.16896\n",
      "val_roc= 0.66219 val_ap= 0.71045\n",
      "Epoch: 0158 train_loss= 0.71885 time= 0.19104\n",
      "val_roc= 0.66238 val_ap= 0.71053\n",
      "Epoch: 0159 train_loss= 0.71991 time= 0.17404\n",
      "val_roc= 0.66249 val_ap= 0.71056\n",
      "Epoch: 0160 train_loss= 0.71762 time= 0.17280\n",
      "val_roc= 0.66267 val_ap= 0.71069\n",
      "Epoch: 0161 train_loss= 0.72075 time= 0.17043\n",
      "val_roc= 0.66270 val_ap= 0.71076\n",
      "Epoch: 0162 train_loss= 0.72061 time= 0.18741\n",
      "val_roc= 0.66278 val_ap= 0.71081\n",
      "Epoch: 0163 train_loss= 0.71778 time= 0.17525\n",
      "val_roc= 0.66283 val_ap= 0.71078\n",
      "Epoch: 0164 train_loss= 0.71881 time= 0.17127\n",
      "val_roc= 0.66288 val_ap= 0.71079\n",
      "Epoch: 0165 train_loss= 0.71838 time= 0.17784\n",
      "val_roc= 0.66287 val_ap= 0.71083\n",
      "Epoch: 0166 train_loss= 0.71995 time= 0.17865\n",
      "val_roc= 0.66307 val_ap= 0.71104\n",
      "Epoch: 0167 train_loss= 0.71827 time= 0.16760\n",
      "val_roc= 0.66326 val_ap= 0.71085\n",
      "Epoch: 0168 train_loss= 0.72077 time= 0.16750\n",
      "val_roc= 0.66330 val_ap= 0.71115\n",
      "Epoch: 0169 train_loss= 0.71815 time= 0.17503\n",
      "val_roc= 0.66352 val_ap= 0.71133\n",
      "Epoch: 0170 train_loss= 0.71916 time= 0.18061\n",
      "val_roc= 0.66361 val_ap= 0.71139\n",
      "Epoch: 0171 train_loss= 0.72095 time= 0.16512\n",
      "val_roc= 0.66372 val_ap= 0.71143\n",
      "Epoch: 0172 train_loss= 0.71783 time= 0.17534\n",
      "val_roc= 0.66388 val_ap= 0.71117\n",
      "Epoch: 0173 train_loss= 0.71820 time= 0.18622\n",
      "val_roc= 0.66407 val_ap= 0.71159\n",
      "Epoch: 0174 train_loss= 0.71693 time= 0.18387\n",
      "val_roc= 0.66423 val_ap= 0.71171\n",
      "Epoch: 0175 train_loss= 0.72021 time= 0.17234\n",
      "val_roc= 0.66445 val_ap= 0.71177\n",
      "Epoch: 0176 train_loss= 0.71745 time= 0.16075\n",
      "val_roc= 0.66458 val_ap= 0.71179\n",
      "Epoch: 0177 train_loss= 0.71737 time= 0.16595\n",
      "val_roc= 0.66465 val_ap= 0.71173\n",
      "Epoch: 0178 train_loss= 0.71725 time= 0.17143\n",
      "val_roc= 0.66484 val_ap= 0.71150\n",
      "Epoch: 0179 train_loss= 0.72213 time= 0.17092\n",
      "val_roc= 0.66510 val_ap= 0.71163\n",
      "Epoch: 0180 train_loss= 0.71612 time= 0.16704\n",
      "val_roc= 0.66527 val_ap= 0.71173\n",
      "Epoch: 0181 train_loss= 0.71861 time= 0.16631\n",
      "val_roc= 0.66541 val_ap= 0.71185\n",
      "Epoch: 0182 train_loss= 0.71801 time= 0.18835\n",
      "val_roc= 0.66560 val_ap= 0.71164\n",
      "Epoch: 0183 train_loss= 0.71772 time= 0.18526\n",
      "val_roc= 0.66580 val_ap= 0.71171\n",
      "Epoch: 0184 train_loss= 0.71649 time= 0.18763\n",
      "val_roc= 0.66586 val_ap= 0.71177\n",
      "Epoch: 0185 train_loss= 0.71644 time= 0.17303\n",
      "val_roc= 0.66578 val_ap= 0.71159\n",
      "Epoch: 0186 train_loss= 0.71812 time= 0.21725\n",
      "val_roc= 0.66612 val_ap= 0.71202\n",
      "Epoch: 0187 train_loss= 0.71732 time= 0.17850\n",
      "val_roc= 0.66624 val_ap= 0.71211\n",
      "Epoch: 0188 train_loss= 0.71954 time= 0.21220\n",
      "val_roc= 0.66656 val_ap= 0.71227\n",
      "Epoch: 0189 train_loss= 0.71722 time= 0.24106\n",
      "val_roc= 0.66702 val_ap= 0.71252\n",
      "Epoch: 0190 train_loss= 0.71574 time= 0.17433\n",
      "val_roc= 0.66721 val_ap= 0.71258\n",
      "Epoch: 0191 train_loss= 0.71737 time= 0.22163\n",
      "val_roc= 0.66702 val_ap= 0.71213\n",
      "Epoch: 0192 train_loss= 0.71726 time= 0.18845\n",
      "val_roc= 0.66699 val_ap= 0.71189\n",
      "Epoch: 0193 train_loss= 0.71891 time= 0.17770\n",
      "val_roc= 0.66684 val_ap= 0.71165\n",
      "Epoch: 0194 train_loss= 0.71845 time= 0.22414\n",
      "val_roc= 0.66709 val_ap= 0.71166\n",
      "Epoch: 0195 train_loss= 0.71817 time= 0.18679\n",
      "val_roc= 0.66711 val_ap= 0.71131\n",
      "Epoch: 0196 train_loss= 0.71636 time= 0.21636\n",
      "val_roc= 0.66708 val_ap= 0.71126\n",
      "Epoch: 0197 train_loss= 0.71677 time= 0.19051\n",
      "val_roc= 0.66718 val_ap= 0.71120\n",
      "Epoch: 0198 train_loss= 0.71858 time= 0.21795\n",
      "val_roc= 0.66728 val_ap= 0.71116\n",
      "Epoch: 0199 train_loss= 0.71583 time= 0.23526\n",
      "val_roc= 0.66741 val_ap= 0.71129\n",
      "Epoch: 0200 train_loss= 0.71924 time= 0.17692\n",
      "val_roc= 0.66757 val_ap= 0.71114\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 1.74399 time= 2.27469\n",
      "val_roc= 0.65181 val_ap= 0.68544\n",
      "Epoch: 0002 train_loss= 1.64662 time= 0.20930\n",
      "val_roc= 0.50000 val_ap= 0.50000\n",
      "Epoch: 0003 train_loss= 788412992.00000 time= 0.20794\n",
      "val_roc= 0.66715 val_ap= 0.69911\n",
      "Epoch: 0004 train_loss= 1.48077 time= 0.20283\n",
      "val_roc= 0.67862 val_ap= 0.71205\n",
      "Epoch: 0005 train_loss= 1.71380 time= 0.17538\n",
      "val_roc= 0.68839 val_ap= 0.71865\n",
      "Epoch: 0006 train_loss= 1.73034 time= 0.22113\n",
      "val_roc= 0.68959 val_ap= 0.71450\n",
      "Epoch: 0007 train_loss= 1.74411 time= 0.16732\n",
      "val_roc= 0.68505 val_ap= 0.68787\n",
      "Epoch: 0008 train_loss= 1.73827 time= 0.18981\n",
      "val_roc= 0.62284 val_ap= 0.61448\n",
      "Epoch: 0009 train_loss= 1.74015 time= 0.22302\n",
      "val_roc= 0.63292 val_ap= 0.60616\n",
      "Epoch: 0010 train_loss= 1.75225 time= 0.18091\n",
      "val_roc= 0.58940 val_ap= 0.58473\n",
      "Epoch: 0011 train_loss= 1.77043 time= 0.22540\n",
      "val_roc= 0.56846 val_ap= 0.56382\n",
      "Epoch: 0012 train_loss= 1.74116 time= 0.23759\n",
      "val_roc= 0.59320 val_ap= 0.58849\n",
      "Epoch: 0013 train_loss= 1.71293 time= 0.20461\n",
      "val_roc= 0.60717 val_ap= 0.59316\n",
      "Epoch: 0014 train_loss= 1.71559 time= 0.18320\n",
      "val_roc= 0.63968 val_ap= 0.60858\n",
      "Epoch: 0015 train_loss= 1.71204 time= 0.18456\n",
      "val_roc= 0.65605 val_ap= 0.64604\n",
      "Epoch: 0016 train_loss= 1.73934 time= 0.17810\n",
      "val_roc= 0.67650 val_ap= 0.68635\n",
      "Epoch: 0017 train_loss= 1.72415 time= 0.23364\n",
      "val_roc= 0.68144 val_ap= 0.70333\n",
      "Epoch: 0018 train_loss= 1.72935 time= 0.19291\n",
      "val_roc= 0.68463 val_ap= 0.70999\n",
      "Epoch: 0019 train_loss= 1.72695 time= 0.20840\n",
      "val_roc= 0.68505 val_ap= 0.71165\n",
      "Epoch: 0020 train_loss= 1.76246 time= 0.22664\n",
      "val_roc= 0.68517 val_ap= 0.71232\n",
      "Epoch: 0021 train_loss= 1.74798 time= 0.17194\n",
      "val_roc= 0.68536 val_ap= 0.71248\n",
      "Epoch: 0022 train_loss= 1.68959 time= 0.20322\n",
      "val_roc= 0.68529 val_ap= 0.71210\n",
      "Epoch: 0023 train_loss= 1.67508 time= 0.19576\n",
      "val_roc= 0.68547 val_ap= 0.71232\n",
      "Epoch: 0024 train_loss= 1.59654 time= 0.19047\n",
      "val_roc= 0.68541 val_ap= 0.71201\n",
      "Epoch: 0025 train_loss= 1.38125 time= 0.24042\n",
      "val_roc= 0.68600 val_ap= 0.71242\n",
      "Epoch: 0026 train_loss= 1.11395 time= 0.16339\n",
      "val_roc= 0.68616 val_ap= 0.71252\n",
      "Epoch: 0027 train_loss= 1.60446 time= 0.21493\n",
      "val_roc= 0.68616 val_ap= 0.71239\n",
      "Epoch: 0028 train_loss= 0.96144 time= 0.17142\n",
      "val_roc= 0.68576 val_ap= 0.71212\n",
      "Epoch: 0029 train_loss= 0.97964 time= 0.20555\n",
      "val_roc= 0.68551 val_ap= 0.71199\n",
      "Epoch: 0030 train_loss= 1.06456 time= 0.19243\n",
      "val_roc= 0.68541 val_ap= 0.71193\n",
      "Epoch: 0031 train_loss= 1.11270 time= 0.17241\n",
      "val_roc= 0.68544 val_ap= 0.71199\n",
      "Epoch: 0032 train_loss= 1.13110 time= 0.20639\n",
      "val_roc= 0.68549 val_ap= 0.71205\n",
      "Epoch: 0033 train_loss= 1.10338 time= 0.17093\n",
      "val_roc= 0.68545 val_ap= 0.71205\n",
      "Epoch: 0034 train_loss= 1.04446 time= 0.22220\n",
      "val_roc= 0.68545 val_ap= 0.71207\n",
      "Epoch: 0035 train_loss= 0.94605 time= 0.18585\n",
      "val_roc= 0.68549 val_ap= 0.71208\n",
      "Epoch: 0036 train_loss= 0.83993 time= 0.19424\n",
      "val_roc= 0.68565 val_ap= 0.71217\n",
      "Epoch: 0037 train_loss= 0.77219 time= 0.23440\n",
      "val_roc= 0.68561 val_ap= 0.71213\n",
      "Epoch: 0038 train_loss= 0.75616 time= 0.22841\n",
      "val_roc= 0.68568 val_ap= 0.71219\n",
      "Epoch: 0039 train_loss= 0.80651 time= 0.20283\n",
      "val_roc= 0.68568 val_ap= 0.71224\n",
      "Epoch: 0040 train_loss= 0.81178 time= 0.22179\n",
      "val_roc= 0.68567 val_ap= 0.71226\n",
      "Epoch: 0041 train_loss= 0.76447 time= 0.16903\n",
      "val_roc= 0.68567 val_ap= 0.71226\n",
      "Epoch: 0042 train_loss= 0.74523 time= 0.21190\n",
      "val_roc= 0.68564 val_ap= 0.71221\n",
      "Epoch: 0043 train_loss= 0.74894 time= 0.21251\n",
      "val_roc= 0.68560 val_ap= 0.71223\n",
      "Epoch: 0044 train_loss= 0.75192 time= 0.18707\n",
      "val_roc= 0.68558 val_ap= 0.71222\n",
      "Epoch: 0045 train_loss= 0.74553 time= 0.21235\n",
      "val_roc= 0.68560 val_ap= 0.71223\n",
      "Epoch: 0046 train_loss= 0.73908 time= 0.17090\n",
      "val_roc= 0.68554 val_ap= 0.71216\n",
      "Epoch: 0047 train_loss= 0.73423 time= 0.19800\n",
      "val_roc= 0.68549 val_ap= 0.71216\n",
      "Epoch: 0048 train_loss= 0.73337 time= 0.19423\n",
      "val_roc= 0.68542 val_ap= 0.71207\n",
      "Epoch: 0049 train_loss= 0.73572 time= 0.17425\n",
      "val_roc= 0.68535 val_ap= 0.71205\n",
      "Epoch: 0050 train_loss= 0.73173 time= 0.21073\n",
      "val_roc= 0.68536 val_ap= 0.71207\n",
      "Epoch: 0051 train_loss= 0.72982 time= 0.17579\n",
      "val_roc= 0.68525 val_ap= 0.71201\n",
      "Epoch: 0052 train_loss= 0.72892 time= 0.22669\n",
      "val_roc= 0.68525 val_ap= 0.71202\n",
      "Epoch: 0053 train_loss= 0.73371 time= 0.19151\n",
      "val_roc= 0.68528 val_ap= 0.71203\n",
      "Epoch: 0054 train_loss= 0.73127 time= 0.17388\n",
      "val_roc= 0.68523 val_ap= 0.71198\n",
      "Epoch: 0055 train_loss= 0.73830 time= 0.25144\n",
      "val_roc= 0.68518 val_ap= 0.71194\n",
      "Epoch: 0056 train_loss= 0.73576 time= 0.16637\n",
      "val_roc= 0.68523 val_ap= 0.71199\n",
      "Epoch: 0057 train_loss= 0.73723 time= 0.19540\n",
      "val_roc= 0.68523 val_ap= 0.71209\n",
      "Epoch: 0058 train_loss= 0.73182 time= 0.21418\n",
      "val_roc= 0.68518 val_ap= 0.71205\n",
      "Epoch: 0059 train_loss= 0.72976 time= 0.18679\n",
      "val_roc= 0.68515 val_ap= 0.71207\n",
      "Epoch: 0060 train_loss= 0.72979 time= 0.22636\n",
      "val_roc= 0.68512 val_ap= 0.71207\n",
      "Epoch: 0061 train_loss= 0.72738 time= 0.16840\n",
      "val_roc= 0.68515 val_ap= 0.71210\n",
      "Epoch: 0062 train_loss= 0.73071 time= 0.19340\n",
      "val_roc= 0.68516 val_ap= 0.71212\n",
      "Epoch: 0063 train_loss= 0.72498 time= 0.18213\n",
      "val_roc= 0.68510 val_ap= 0.71207\n",
      "Epoch: 0064 train_loss= 0.72591 time= 0.21311\n",
      "val_roc= 0.68508 val_ap= 0.71201\n",
      "Epoch: 0065 train_loss= 0.72478 time= 0.23902\n",
      "val_roc= 0.68513 val_ap= 0.71205\n",
      "Epoch: 0066 train_loss= 0.72507 time= 0.18039\n",
      "val_roc= 0.68513 val_ap= 0.71205\n",
      "Epoch: 0067 train_loss= 0.72390 time= 0.21769\n",
      "val_roc= 0.68506 val_ap= 0.71203\n",
      "Epoch: 0068 train_loss= 0.72663 time= 0.21078\n",
      "val_roc= 0.68509 val_ap= 0.71204\n",
      "Epoch: 0069 train_loss= 0.72266 time= 0.17992\n",
      "val_roc= 0.68518 val_ap= 0.71216\n",
      "Epoch: 0070 train_loss= 0.72432 time= 0.21995\n",
      "val_roc= 0.68528 val_ap= 0.71220\n",
      "Epoch: 0071 train_loss= 0.72265 time= 0.16977\n",
      "val_roc= 0.68531 val_ap= 0.71219\n",
      "Epoch: 0072 train_loss= 0.72301 time= 0.23207\n",
      "val_roc= 0.68531 val_ap= 0.71215\n",
      "Epoch: 0073 train_loss= 0.72328 time= 0.19181\n",
      "val_roc= 0.68534 val_ap= 0.71222\n",
      "Epoch: 0074 train_loss= 0.72101 time= 0.17160\n",
      "val_roc= 0.68509 val_ap= 0.71203\n",
      "Epoch: 0075 train_loss= 0.72185 time= 0.21009\n",
      "val_roc= 0.68513 val_ap= 0.71211\n",
      "Epoch: 0076 train_loss= 0.72359 time= 0.17526\n",
      "val_roc= 0.68508 val_ap= 0.71212\n",
      "Epoch: 0077 train_loss= 0.72122 time= 0.21880\n",
      "val_roc= 0.68495 val_ap= 0.71201\n",
      "Epoch: 0078 train_loss= 0.72214 time= 0.19442\n",
      "val_roc= 0.68500 val_ap= 0.71205\n",
      "Epoch: 0079 train_loss= 0.72126 time= 0.19985\n",
      "val_roc= 0.68502 val_ap= 0.71197\n",
      "Epoch: 0080 train_loss= 0.72201 time= 0.25701\n",
      "val_roc= 0.68505 val_ap= 0.71195\n",
      "Epoch: 0081 train_loss= 0.71944 time= 0.17399\n",
      "val_roc= 0.68508 val_ap= 0.71199\n",
      "Epoch: 0082 train_loss= 0.72298 time= 0.22481\n",
      "val_roc= 0.68503 val_ap= 0.71195\n",
      "Epoch: 0083 train_loss= 0.72026 time= 0.21162\n",
      "val_roc= 0.68496 val_ap= 0.71193\n",
      "Epoch: 0084 train_loss= 0.72000 time= 0.18960\n",
      "val_roc= 0.68505 val_ap= 0.71196\n",
      "Epoch: 0085 train_loss= 0.71967 time= 0.20550\n",
      "val_roc= 0.68513 val_ap= 0.71195\n",
      "Epoch: 0086 train_loss= 0.71879 time= 0.18537\n",
      "val_roc= 0.68506 val_ap= 0.71189\n",
      "Epoch: 0087 train_loss= 0.72045 time= 0.18998\n",
      "val_roc= 0.68528 val_ap= 0.71206\n",
      "Epoch: 0088 train_loss= 0.72177 time= 0.21124\n",
      "val_roc= 0.68538 val_ap= 0.71210\n",
      "Epoch: 0089 train_loss= 0.71883 time= 0.18347\n",
      "val_roc= 0.68531 val_ap= 0.71207\n",
      "Epoch: 0090 train_loss= 0.72147 time= 0.22470\n",
      "val_roc= 0.68535 val_ap= 0.71214\n",
      "Epoch: 0091 train_loss= 0.71824 time= 0.20985\n",
      "val_roc= 0.68538 val_ap= 0.71215\n",
      "Epoch: 0092 train_loss= 0.72023 time= 0.17474\n",
      "val_roc= 0.68535 val_ap= 0.71214\n",
      "Epoch: 0093 train_loss= 0.71781 time= 0.23423\n",
      "val_roc= 0.68547 val_ap= 0.71215\n",
      "Epoch: 0094 train_loss= 0.71831 time= 0.17119\n",
      "val_roc= 0.68534 val_ap= 0.71197\n",
      "Epoch: 0095 train_loss= 0.71844 time= 0.19887\n",
      "val_roc= 0.68544 val_ap= 0.71209\n",
      "Epoch: 0096 train_loss= 0.71935 time= 0.22671\n",
      "val_roc= 0.68534 val_ap= 0.71194\n",
      "Epoch: 0097 train_loss= 0.72030 time= 0.18492\n",
      "val_roc= 0.68547 val_ap= 0.71208\n",
      "Epoch: 0098 train_loss= 0.71761 time= 0.20410\n",
      "val_roc= 0.68542 val_ap= 0.71209\n",
      "Epoch: 0099 train_loss= 0.72027 time= 0.20241\n",
      "val_roc= 0.68542 val_ap= 0.71213\n",
      "Epoch: 0100 train_loss= 0.71969 time= 0.18009\n",
      "val_roc= 0.68541 val_ap= 0.71220\n",
      "Epoch: 0101 train_loss= 0.71784 time= 0.21763\n",
      "val_roc= 0.68523 val_ap= 0.71213\n",
      "Epoch: 0102 train_loss= 0.71698 time= 0.19734\n",
      "val_roc= 0.68521 val_ap= 0.71197\n",
      "Epoch: 0103 train_loss= 0.71906 time= 0.17756\n",
      "val_roc= 0.68513 val_ap= 0.71184\n",
      "Epoch: 0104 train_loss= 0.71776 time= 0.24338\n",
      "val_roc= 0.68505 val_ap= 0.71174\n",
      "Epoch: 0105 train_loss= 0.71823 time= 0.17921\n",
      "val_roc= 0.68503 val_ap= 0.71159\n",
      "Epoch: 0106 train_loss= 0.71817 time= 0.26491\n",
      "val_roc= 0.68483 val_ap= 0.71147\n",
      "Epoch: 0107 train_loss= 0.71716 time= 0.24264\n",
      "val_roc= 0.68497 val_ap= 0.71159\n",
      "Epoch: 0108 train_loss= 0.71823 time= 0.18744\n",
      "val_roc= 0.68474 val_ap= 0.71154\n",
      "Epoch: 0109 train_loss= 0.71741 time= 0.20184\n",
      "val_roc= 0.68476 val_ap= 0.71158\n",
      "Epoch: 0110 train_loss= 0.71806 time= 0.17796\n",
      "val_roc= 0.68463 val_ap= 0.71152\n",
      "Epoch: 0111 train_loss= 0.71739 time= 0.17710\n",
      "val_roc= 0.68463 val_ap= 0.71149\n",
      "Epoch: 0112 train_loss= 0.71729 time= 0.22662\n",
      "val_roc= 0.68456 val_ap= 0.71150\n",
      "Epoch: 0113 train_loss= 0.71569 time= 0.18743\n",
      "val_roc= 0.68456 val_ap= 0.71156\n",
      "Epoch: 0114 train_loss= 0.71724 time= 0.17158\n",
      "val_roc= 0.68418 val_ap= 0.71140\n",
      "Epoch: 0115 train_loss= 0.71606 time= 0.24047\n",
      "val_roc= 0.68399 val_ap= 0.71131\n",
      "Epoch: 0116 train_loss= 0.71997 time= 0.16521\n",
      "val_roc= 0.68401 val_ap= 0.71146\n",
      "Epoch: 0117 train_loss= 0.71487 time= 0.20058\n",
      "val_roc= 0.68414 val_ap= 0.71164\n",
      "Epoch: 0118 train_loss= 0.71385 time= 0.21334\n",
      "val_roc= 0.68411 val_ap= 0.71171\n",
      "Epoch: 0119 train_loss= 0.71658 time= 0.18973\n",
      "val_roc= 0.68418 val_ap= 0.71168\n",
      "Epoch: 0120 train_loss= 0.71660 time= 0.20016\n",
      "val_roc= 0.68421 val_ap= 0.71173\n",
      "Epoch: 0121 train_loss= 0.71640 time= 0.18709\n",
      "val_roc= 0.68424 val_ap= 0.71183\n",
      "Epoch: 0122 train_loss= 0.71403 time= 0.18839\n",
      "val_roc= 0.68447 val_ap= 0.71205\n",
      "Epoch: 0123 train_loss= 0.71464 time= 0.20899\n",
      "val_roc= 0.68428 val_ap= 0.71221\n",
      "Epoch: 0124 train_loss= 0.71605 time= 0.18463\n",
      "val_roc= 0.68411 val_ap= 0.71226\n",
      "Epoch: 0125 train_loss= 0.71423 time= 0.21761\n",
      "val_roc= 0.68386 val_ap= 0.71211\n",
      "Epoch: 0126 train_loss= 0.71328 time= 0.21304\n",
      "val_roc= 0.68349 val_ap= 0.71199\n",
      "Epoch: 0127 train_loss= 0.71493 time= 0.17452\n",
      "val_roc= 0.68359 val_ap= 0.71226\n",
      "Epoch: 0128 train_loss= 0.71601 time= 0.22858\n",
      "val_roc= 0.68360 val_ap= 0.71263\n",
      "Epoch: 0129 train_loss= 0.71438 time= 0.21782\n",
      "val_roc= 0.68249 val_ap= 0.71216\n",
      "Epoch: 0130 train_loss= 0.71182 time= 0.19182\n",
      "val_roc= 0.67977 val_ap= 0.71088\n",
      "Epoch: 0131 train_loss= 0.71526 time= 0.24629\n",
      "val_roc= 0.67925 val_ap= 0.71018\n",
      "Epoch: 0132 train_loss= 0.71288 time= 0.18170\n",
      "val_roc= 0.67925 val_ap= 0.71001\n",
      "Epoch: 0133 train_loss= 0.71297 time= 0.19319\n",
      "val_roc= 0.67873 val_ap= 0.70930\n",
      "Epoch: 0134 train_loss= 0.71289 time= 0.18247\n",
      "val_roc= 0.67847 val_ap= 0.70932\n",
      "Epoch: 0135 train_loss= 0.71256 time= 0.16950\n",
      "val_roc= 0.67656 val_ap= 0.70766\n",
      "Epoch: 0136 train_loss= 0.70907 time= 0.23331\n",
      "val_roc= 0.67280 val_ap= 0.70534\n",
      "Epoch: 0137 train_loss= 0.71270 time= 0.16813\n",
      "val_roc= 0.67444 val_ap= 0.70625\n",
      "Epoch: 0138 train_loss= 0.71395 time= 0.20965\n",
      "val_roc= 0.67313 val_ap= 0.70493\n",
      "Epoch: 0139 train_loss= 0.71079 time= 0.19211\n",
      "val_roc= 0.67117 val_ap= 0.70389\n",
      "Epoch: 0140 train_loss= 0.71094 time= 0.18367\n",
      "val_roc= 0.66971 val_ap= 0.70296\n",
      "Epoch: 0141 train_loss= 0.71201 time= 0.22901\n",
      "val_roc= 0.67030 val_ap= 0.70328\n",
      "Epoch: 0142 train_loss= 0.71066 time= 0.16685\n",
      "val_roc= 0.67076 val_ap= 0.70298\n",
      "Epoch: 0143 train_loss= 0.71267 time= 0.21378\n",
      "val_roc= 0.67049 val_ap= 0.70317\n",
      "Epoch: 0144 train_loss= 0.70920 time= 0.19656\n",
      "val_roc= 0.67085 val_ap= 0.70324\n",
      "Epoch: 0145 train_loss= 0.70924 time= 0.16940\n",
      "val_roc= 0.67075 val_ap= 0.70244\n",
      "Epoch: 0146 train_loss= 0.71221 time= 0.21538\n",
      "val_roc= 0.67071 val_ap= 0.70313\n",
      "Epoch: 0147 train_loss= 0.71182 time= 0.17550\n",
      "val_roc= 0.67105 val_ap= 0.70350\n",
      "Epoch: 0148 train_loss= 0.71165 time= 0.21110\n",
      "val_roc= 0.66942 val_ap= 0.70213\n",
      "Epoch: 0149 train_loss= 0.71090 time= 0.20444\n",
      "val_roc= 0.66896 val_ap= 0.70105\n",
      "Epoch: 0150 train_loss= 0.70810 time= 0.18049\n",
      "val_roc= 0.66841 val_ap= 0.70062\n",
      "Epoch: 0151 train_loss= 0.70830 time= 0.20245\n",
      "val_roc= 0.66747 val_ap= 0.69977\n",
      "Epoch: 0152 train_loss= 0.70936 time= 0.17792\n",
      "val_roc= 0.66734 val_ap= 0.70001\n",
      "Epoch: 0153 train_loss= 0.70801 time= 0.22956\n",
      "val_roc= 0.66673 val_ap= 0.69970\n",
      "Epoch: 0154 train_loss= 0.70770 time= 0.22292\n",
      "val_roc= 0.66557 val_ap= 0.69891\n",
      "Epoch: 0155 train_loss= 0.70816 time= 0.17511\n",
      "val_roc= 0.66547 val_ap= 0.69856\n",
      "Epoch: 0156 train_loss= 0.70611 time= 0.19306\n",
      "val_roc= 0.66401 val_ap= 0.69803\n",
      "Epoch: 0157 train_loss= 0.70866 time= 0.22260\n",
      "val_roc= 0.66446 val_ap= 0.69842\n",
      "Epoch: 0158 train_loss= 0.70853 time= 0.17747\n",
      "val_roc= 0.66481 val_ap= 0.69910\n",
      "Epoch: 0159 train_loss= 0.70924 time= 0.33756\n",
      "val_roc= 0.66618 val_ap= 0.70050\n",
      "Epoch: 0160 train_loss= 0.70903 time= 0.20147\n",
      "val_roc= 0.66833 val_ap= 0.70186\n",
      "Epoch: 0161 train_loss= 0.70852 time= 0.17937\n",
      "val_roc= 0.66887 val_ap= 0.70261\n",
      "Epoch: 0162 train_loss= 0.71013 time= 0.44183\n",
      "val_roc= 0.66745 val_ap= 0.70122\n",
      "Epoch: 0163 train_loss= 0.70715 time= 0.17616\n",
      "val_roc= 0.66648 val_ap= 0.70067\n",
      "Epoch: 0164 train_loss= 0.70816 time= 0.23660\n",
      "val_roc= 0.66530 val_ap= 0.69958\n",
      "Epoch: 0165 train_loss= 0.70809 time= 0.22658\n",
      "val_roc= 0.66479 val_ap= 0.69937\n",
      "Epoch: 0166 train_loss= 0.70802 time= 0.18740\n",
      "val_roc= 0.66523 val_ap= 0.69961\n",
      "Epoch: 0167 train_loss= 0.70854 time= 0.26373\n",
      "val_roc= 0.66660 val_ap= 0.70032\n",
      "Epoch: 0168 train_loss= 0.70556 time= 0.18144\n",
      "val_roc= 0.66815 val_ap= 0.70151\n",
      "Epoch: 0169 train_loss= 0.70955 time= 0.23383\n",
      "val_roc= 0.66919 val_ap= 0.70220\n",
      "Epoch: 0170 train_loss= 0.70729 time= 0.19319\n",
      "val_roc= 0.67003 val_ap= 0.70268\n",
      "Epoch: 0171 train_loss= 0.70764 time= 0.23069\n",
      "val_roc= 0.66990 val_ap= 0.70244\n",
      "Epoch: 0172 train_loss= 0.70744 time= 0.19932\n",
      "val_roc= 0.66959 val_ap= 0.70230\n",
      "Epoch: 0173 train_loss= 0.70802 time= 0.16620\n",
      "val_roc= 0.66956 val_ap= 0.70223\n",
      "Epoch: 0174 train_loss= 0.70657 time= 0.22153\n",
      "val_roc= 0.66911 val_ap= 0.70207\n",
      "Epoch: 0175 train_loss= 0.70813 time= 0.19677\n",
      "val_roc= 0.66871 val_ap= 0.70171\n",
      "Epoch: 0176 train_loss= 0.70649 time= 0.20046\n",
      "val_roc= 0.66776 val_ap= 0.70092\n",
      "Epoch: 0177 train_loss= 0.70720 time= 0.20359\n",
      "val_roc= 0.66713 val_ap= 0.70018\n",
      "Epoch: 0178 train_loss= 0.70704 time= 0.17982\n",
      "val_roc= 0.66737 val_ap= 0.70050\n",
      "Epoch: 0179 train_loss= 0.70725 time= 0.39772\n",
      "val_roc= 0.66802 val_ap= 0.70120\n",
      "Epoch: 0180 train_loss= 0.70676 time= 0.16761\n",
      "val_roc= 0.66778 val_ap= 0.70127\n",
      "Epoch: 0181 train_loss= 0.70757 time= 0.19137\n",
      "val_roc= 0.66816 val_ap= 0.70188\n",
      "Epoch: 0182 train_loss= 0.70681 time= 0.22897\n",
      "val_roc= 0.66849 val_ap= 0.70185\n",
      "Epoch: 0183 train_loss= 0.70567 time= 0.16925\n",
      "val_roc= 0.66647 val_ap= 0.70032\n",
      "Epoch: 0184 train_loss= 0.70512 time= 0.26659\n",
      "val_roc= 0.66463 val_ap= 0.69864\n",
      "Epoch: 0185 train_loss= 0.70783 time= 0.17608\n",
      "val_roc= 0.66343 val_ap= 0.69782\n",
      "Epoch: 0186 train_loss= 0.70664 time= 0.19048\n",
      "val_roc= 0.66209 val_ap= 0.69703\n",
      "Epoch: 0187 train_loss= 0.70624 time= 0.24985\n",
      "val_roc= 0.66118 val_ap= 0.69637\n",
      "Epoch: 0188 train_loss= 0.70584 time= 0.17364\n",
      "val_roc= 0.66164 val_ap= 0.69656\n",
      "Epoch: 0189 train_loss= 0.70474 time= 0.25082\n",
      "val_roc= 0.66168 val_ap= 0.69652\n",
      "Epoch: 0190 train_loss= 0.70662 time= 0.17760\n",
      "val_roc= 0.66325 val_ap= 0.69742\n",
      "Epoch: 0191 train_loss= 0.70522 time= 0.23327\n",
      "val_roc= 0.66410 val_ap= 0.69771\n",
      "Epoch: 0192 train_loss= 0.70712 time= 0.20239\n",
      "val_roc= 0.66418 val_ap= 0.69798\n",
      "Epoch: 0193 train_loss= 0.70365 time= 0.22110\n",
      "val_roc= 0.66372 val_ap= 0.69789\n",
      "Epoch: 0194 train_loss= 0.70598 time= 0.18845\n",
      "val_roc= 0.66307 val_ap= 0.69757\n",
      "Epoch: 0195 train_loss= 0.70435 time= 0.18558\n",
      "val_roc= 0.66158 val_ap= 0.69628\n",
      "Epoch: 0196 train_loss= 0.70552 time= 0.23773\n",
      "val_roc= 0.66001 val_ap= 0.69498\n",
      "Epoch: 0197 train_loss= 0.70437 time= 0.16846\n",
      "val_roc= 0.65872 val_ap= 0.69400\n",
      "Epoch: 0198 train_loss= 0.70439 time= 0.23592\n",
      "val_roc= 0.65752 val_ap= 0.69355\n",
      "Epoch: 0199 train_loss= 0.70383 time= 0.31032\n",
      "val_roc= 0.65690 val_ap= 0.69309\n",
      "Epoch: 0200 train_loss= 0.70450 time= 0.17022\n",
      "val_roc= 0.65619 val_ap= 0.69253\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 1.76734 time= 2.31124\n",
      "val_roc= 0.67512 val_ap= 0.69492\n",
      "Epoch: 0002 train_loss= 1.66845 time= 0.21508\n",
      "val_roc= 0.49620 val_ap= 0.49810\n",
      "Epoch: 0003 train_loss= nan time= 0.18066\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cad3da40b5e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mplaceholders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mval_roc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_roc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_edges_false\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_roc=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_roc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_ap=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_ap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Done_Results/GAE_VGAE_Results/GVAE_Pubmed_36/linear_gae/evaluation.py\u001b[0m in \u001b[0;36mget_roc_score\u001b[0;34m(edges_pos, edges_neg, emb)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlabels_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mroc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0map_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     if y_type == \"multiclass\" or (y_type == \"binary\" and\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# The entire training+test process is repeated nb_run times\n",
    "for seed_i in np.arange(nb_run):\n",
    "    seed=seed_i\n",
    "    if task == 'link_prediction' :\n",
    "        if verbose:\n",
    "            print(\"Masking test edges...\")\n",
    "        adj, val_edges, val_edges_false, test_edges, test_edges_false = \\\n",
    "        mask_test_edges(adj_init, seed,prop_test, prop_val)\n",
    "        \n",
    "    # Start computation of running times\n",
    "    t_start = time.time()\n",
    "\n",
    "    if features_used:\n",
    "        features = features_init\n",
    "        \n",
    "    # Preprocessing and initialization\n",
    "    if verbose:\n",
    "        print(\"Preprocessing and Initializing...\")\n",
    "        \n",
    "    # Compute number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "    # If features are not used, replace feature matrix by identity matrix\n",
    "    if not features_used:\n",
    "        features = sp.identity(adj.shape[0])\n",
    "    # Preprocessing on node features\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "    \n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ())\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    if model_name == 'gcn_ae':\n",
    "        # Standard Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif model_name == 'gcn_vae':\n",
    "        # Standard Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                            features_nonzero)\n",
    "    elif model_name == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif model_name == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes,\n",
    "                               features_nonzero)\n",
    "    elif model_name == 'deep12_gcn_ae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = Deep12GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif model_name == 'deep_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Variational Autoencoder\n",
    "        model = DeepGCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                                features_nonzero)\n",
    "    elif model_name == 'deep6_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = Deep6GCNModelVAE(placeholders, num_features, num_nodes,features_nonzero)\n",
    "    elif model_name == 'deep12_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = Deep12GCNModelVAE(placeholders, num_features, num_nodes,features_nonzero)\n",
    "    elif model_name == 'deep18_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = Deep18GCNModelVAE(placeholders, num_features, num_nodes,features_nonzero)\n",
    "    elif model_name == 'deep36_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = Deep36GCNModelVAE(placeholders, num_features, num_nodes,features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "    # Optimizer\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0]\n",
    "                                                - adj.sum()) * 2)\n",
    "    with tf.name_scope('optimizer'):\n",
    "        # Optimizer for Non-Variational Autoencoders\n",
    "        if model_name in ('gcn_ae', 'linear_ae', 'deep_gcn_ae','deep12_gcn_ae'):\n",
    "            opt = OptimizerAE(preds = model.reconstructions,\n",
    "                              labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                            validate_indices = False), [-1]),\n",
    "                              pos_weight = pos_weight,\n",
    "                              norm = norm)\n",
    "\n",
    "            # Optimizer for Variational Autoencoders\n",
    "        elif model_name in ('gcn_vae', 'linear_vae', 'deep_gcn_vae','deep6_gcn_vae','deep12_gcn_vae','deep18_gcn_vae','deep36_gcn_vae'):\n",
    "            opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                               labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                             validate_indices = False), [-1]),\n",
    "                               model = model,\n",
    "                               num_nodes = num_nodes,\n",
    "                               pos_weight = pos_weight,\n",
    "                               norm = norm)\n",
    "\n",
    "    # Normalization and preprocessing on adjacency matrix\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = sparse_to_tuple(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # Initialize TF session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Model training\n",
    "    if verbose:\n",
    "        print(\"Training...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Flag to compute running time for each epoch\n",
    "        t = time.time()\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_label, features,\n",
    "                                        placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: dropout})\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.accuracy],\n",
    "                        feed_dict = feed_dict)\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if verbose:\n",
    "            # Display epoch information\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "            # Validation, for Link Prediction\n",
    "            if not kcore and validation and task == 'link_prediction':\n",
    "                feed_dict.update({placeholders['dropout']: 0})\n",
    "                emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "                feed_dict.update({placeholders['dropout']: dropout})\n",
    "                val_roc, val_ap = get_roc_score(val_edges, val_edges_false, emb)\n",
    "                print(\"val_roc=\", \"{:.5f}\".format(val_roc), \"val_ap=\", \"{:.5f}\".format(val_ap))\n",
    "\n",
    "    # Flag to compute Graph AE/VAE training time\n",
    "    t_model = time.time()\n",
    "\n",
    "    # Compute embedding\n",
    "\n",
    "    # Get embedding from model\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "    \n",
    "    # Compute mean total running time\n",
    "    mean_time.append(time.time() - t_start)\n",
    "    \n",
    "    # Test model\n",
    "    if verbose:\n",
    "        print(\"Testing model...\")\n",
    "    # Link Prediction: classification edges/non-edges\n",
    "    if task == 'link_prediction':\n",
    "        # Get ROC and AP scores\n",
    "        roc_score, ap_score = get_roc_score(test_edges, test_edges_false, emb)\n",
    "        # Report scores\n",
    "        mean_roc.append(roc_score)\n",
    "        mean_ap.append(ap_score)\n",
    "\n",
    "mean_time_=np.array(mean_time)\n",
    "write_to_csv(mean_time_.reshape(1,len(mean_time_)),output_path+p_model_name+\"_time_\"+str(features_used)+\".csv\")\n",
    "\n",
    "mean_roc_=np.array(mean_roc)\n",
    "write_to_csv(mean_roc_.reshape(1,len(mean_roc_)),output_path+p_model_name+\"_roc_\"+str(features_used)+\".csv\")\n",
    "\n",
    "mean_ap_=np.array(mean_ap)\n",
    "write_to_csv(mean_ap_.reshape(1,len(mean_ap_)),output_path+p_model_name+\"_ap_\"+str(features_used)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
